---
title: "Hotels_CF"
output: html_document
---

```{r}
data <- read.csv('Datafiniti_Hotel_Reviews.csv')
```

```{r}
library(tidyr)

# keep only hotel names, reviewer usernames, and their rating
df <- subset(data, select=c('name', 'reviews.username', 'reviews.rating'))
# keep only hotels that have a minimum of 5 ratings
cf_df <- df[df$name %in% names(which(table(df$name) >= 5)), ]
# remove "A verified traveler", "Anonymous", "A Traveler", and "A TripAdvisor Member" to avoid multiple people counting as one
cf_df <- subset(cf_df, !(reviews.username %in% c("A verified traveler", "Anonymous", "A Traveler", "A TripAdvisor Member")))

# If a reviewer that rated the same hotel twice, take the mean of their ratings
cf_df <- group_by(cf_df, name, reviews.username)
cf_df <- summarize(cf_df, reviews.rating = mean(reviews.rating))

# pivot table to be ready for CF
cf_table <- pivot_wider(cf_df, names_from = name, values_from = reviews.rating, id_cols = reviews.username)

# make the users the rows
rownames(cf_table) <- cf_table$reviews.username
cf_table$reviews.username <- NULL
```
```{r}
```

Item-Item
506 total hotels

```{r}
item_item_cf <- function(df, target, sim_metric, k) {
  
  target_row <- df[target, ]
  
  # exit function if target isn't in data
  if (!(target %in% rownames(df))) {
    print(paste("Error:", target, "not found in the dataset."))
    return(NULL)
  }
  
  # scale columns
  data_mat <- as.matrix(df)
  data_mat_scaled <- sweep(data_mat, 2, colMeans(data_mat, na.rm = TRUE), "-")
  
  # pairwise similarity scores
  sim_scores <- matrix(0, ncol = ncol(data_mat_scaled), nrow = ncol(data_mat_scaled))
  for (i in 1:(ncol(data_mat_scaled) - 1)) {
    for (j in (i + 1):ncol(data_mat_scaled)) {
      ItemA <- data_mat_scaled[, i]
      ItemB <- data_mat_scaled[, j]
      shared <- !is.na(ItemA) & !is.na(ItemB)
      
      if (sim_metric == "Cosine") {
        sim <- sum(ItemA[shared] * ItemB[shared]) / 
               (sqrt(sum(ItemA[shared]^2)) * sqrt(sum(ItemB[shared]^2)))
      } else if (sim_metric == "L2") {
        sim <- -sqrt(sum((ItemA[shared] - ItemB[shared])^2))
      }
      
      sim_scores[i, j] <- sim
      sim_scores[j, i] <- sim
    }
  }
  
  # min max scale
  diag(sim_scores) <- NA
  sim_scaled <- apply(sim_scores, 2,
                      function(x) (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
  sim_scaled <- as.data.frame(sim_scaled)
  # rename rows and columns to item names
  colnames(sim_scaled) <- colnames(df)
  rownames(sim_scaled) <- colnames(df)
  
  # weighted averages
  for (item in colnames(df)) {
    target_rating <- df[target, item]
    if (is.na(target_rating)) {
      weight_avg <- 0
      # get the k most similar items to the current item
      k_similar <- head(sim_scaled[order(-sim_scaled[[item]]), item], k)
      sim_items <- names(k_similar)
      for (sim_item in sim_items) {
        # use their mean if any of the similar students have a NaN value for that rating
        if (is.na(df[target, sim_item])) {
          weight_avg <- mean(df[[sim_item]], na.rm = TRUE)
          break
        }
        weight_avg <- weight_avg + (k_similar[sim_item] * df[target, sim_item])
      }
  
      if (weight_avg != mean(df[[sim_item]], na.rm = TRUE)) {
        weight_avg <- weight_avg / sum(k_similar, na.rm = TRUE)
      }
  
      print(paste0(target, " pred. rating for ", item, ": ", round(weight_avg,2)))
    }
  }

  return(NULL)
}
```

